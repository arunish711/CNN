{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cifar10\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10.data_path = \"data/CIFAR-10/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "cifar10.maybe_download_and_extract()  ## Extracting the CIFAR-10 datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: data/CIFAR-10/cifar-10-batches-py/batches.meta\n"
     ]
    }
   ],
   "source": [
    "class_names = cifar10.load_class_names()  ## Getting to know about the 10 classes that is there in the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_1\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_2\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_3\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_4\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_5\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/test_batch\n"
     ]
    }
   ],
   "source": [
    "images_train, cls_train, labels_train = cifar10.load_training_data()  ## Lodaing the training dataset\n",
    "images_test, cls_test, labels_test = cifar10.load_test_data()  ## Loading the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are defining the architecture of our CNN for CIFAR-10 dataset.\n",
    "## n_cov1 , n_cov2 --> convulational layers.\n",
    "## cov1_k , cov2_k --> filters of convulational layers 1 and 2 respectively.\n",
    "## n_hidden--> Dense Layer\n",
    "## n_out--> Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width=32     \n",
    "input_height=32\n",
    "input_channels=3\n",
    "\n",
    "input_pixels=32*32*3\n",
    "n_conv1=32\n",
    "n_conv2=64\n",
    "conv1_k=5\n",
    "conv2_k=5\n",
    "\n",
    "max_pool1_k=2\n",
    "max_pool2_k=2\n",
    "\n",
    "n_hidden=1024\n",
    "n_out=10\n",
    "\n",
    "stride_conv1=1\n",
    "stride_conv2=1\n",
    "\n",
    "input_size_to_hidden=(input_width//(max_pool1_k*max_pool2_k))*(input_height//(max_pool1_k*max_pool2_k))*n_conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we are reshaping our training and testing data into from 3D to 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train=np.reshape(images_train,(images_train.shape[0],input_pixels))\n",
    "images_test=np.reshape(images_test,(images_test.shape[0],input_pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are initialising the different weights and biases of different layers by the random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights={\n",
    "    \n",
    "    \"wc1\" : tf.Variable(tf.random_normal([conv1_k,conv1_k,input_channels,n_conv1])),\n",
    "    \"wc2\" : tf.Variable(tf.random_normal([conv2_k,conv2_k,n_conv1,n_conv2])),\n",
    "    \"wh1\" : tf.Variable(tf.random_normal([input_size_to_hidden,n_hidden])),\n",
    "    \"wo\"  : tf.Variable(tf.random_normal([n_hidden,n_out]))\n",
    "}\n",
    "\n",
    "biases={\n",
    "    \n",
    "    \"bc1\" : tf.Variable(tf.random_normal([n_conv1])),\n",
    "    \"bc2\" : tf.Variable(tf.random_normal([n_conv2])),\n",
    "    \"bh1\" : tf.Variable(tf.random_normal([n_hidden])),\n",
    "    \"bo\"  : tf.Variable(tf.random_normal([n_out]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now here data is being acted by the convulational layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(x,weights,bias,stride=1):\n",
    "    out=tf.nn.conv2d(x,weights,padding=\"SAME\",strides=[1,stride,stride,1])\n",
    "    out=tf.nn.bias_add(out,bias)\n",
    "    out=tf.nn.relu(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pooling Layer is added after the convulational layer which basically tries to reduce the size of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling(x,k=2):\n",
    "    return tf.nn.max_pool(x,padding=\"SAME\",ksize=[1,k,k,1],strides=[1,k,k,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In this function we are doing forward propagation .\n",
    "## At first data is passed through the two convulational layers and pooling layers respectively .\n",
    "## Then we are reshaping our data before passing through the dense layer.\n",
    "## After data is being passed through the dense layer we are getting the output which is a vector of size 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(x,weights,biases):\n",
    "    \n",
    "    x=tf.reshape(x,shape=[-1,input_height,input_width,input_channels])\n",
    "    conv1=conv(x,weights[\"wc1\"],biases[\"bc1\"],stride_conv1)\n",
    "    conv1_pool=max_pooling(conv1,max_pool1_k)\n",
    "    conv2=conv(conv1_pool,weights[\"wc2\"],biases[\"bc2\"],stride_conv2)\n",
    "    conv2_pool=max_pooling(conv2,max_pool2_k)\n",
    "    \n",
    "    hidden_input=tf.reshape(conv2_pool,shape=[-1,input_size_to_hidden])\n",
    "    hidden_output_before_activation=tf.add(tf.matmul(hidden_input,weights[\"wh1\"]),biases[\"bh1\"])\n",
    "    hidden_output=tf.nn.relu(hidden_output_before_activation)\n",
    "    output=tf.add(tf.matmul(hidden_output,weights[\"wo\"]),biases[\"bo\"])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here in this cell we are sending the data for training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(\"float\",[None,input_pixels])\n",
    "y=tf.placeholder(tf.int32,[None,n_out])\n",
    "\n",
    "pred=cnn(x,weights,biases)\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred,labels=y))\n",
    "\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "optimize=optimizer.minimize(cost)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we are backpropogating our code to find the best values of weights and biases which gives minimum cost . T\n",
    "##  We are sending our data in batches in every iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "\n",
    "for i in range(25):\n",
    "    num_batches=int(labels_train.shape[0]/batch_size)\n",
    "    for j in range(num_batches):\n",
    "        c,_=sess.run([cost,optimize],feed_dict={x:images_train,y:labels_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now Finally! we got our predictions and we now are comparing with the testing output and checking the accuracy of\n",
    "## our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.argmax(pred, 1)\n",
    "correct_labels=[]\n",
    "for i in range(labels_test.shape[0]):\n",
    "    for j in range(labels_test.shape[1]):\n",
    "        if(labels_test[i][j]==1):\n",
    "            correct_labels.append(j)\n",
    "\n",
    "correct_labels=np.array(correct_labels)\n",
    "correct_labels=np.reshape(correct_labels,(labels_test.shape[0]))\n",
    "\n",
    "correct_predictions = tf.equal(predictions, correct_labels)\n",
    "predictions,correct_preds = sess.run([predictions,correct_predictions], feed_dict={x:images_test,y:labels_test})\n",
    "correct_preds.sum()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
